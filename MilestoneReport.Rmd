---
title: "Exploratory Analysis - Milestone Report"
author: "Luis Padua"
date: "January 29, 2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(dplyr)
library(stringr)
library(tm)
library(knitr)
library(ggplot2)
```

## Summary

This Report shows the initial work necessary to complete the Capstone Project of the Data Science course of John Hopkins University. The project consists of creating a text predictor and here it will be described the exploratory analysis done on the training dataset. The dataset can be downloaded at

[Capstone Dataset](https://d396qusza40orc.cloudfront.net/dsscapstone/dataset/Coursera-SwiftKey.zip)

I'm going to work with the English languages files, the dataset consists of 3 files that can be summarized as the following.

```{r loaddata, cache=TRUE}
readData <- function(fileName) {
        # it is important to set "rb" to READ BINARY
        con <- file(fileName, "rb")
        #skipNul equals TRUE is necessary to Twitter data
        data <- readLines(con, skipNul = TRUE)
        data <- iconv(data, 'UTF-8', 'ASCII', sub='') #remove the UTF-8 strange character to a blank space
        close(con)
        return(data)
}

blogs <- readData("en_US/en_US.blogs.txt")
news <- readData("en_US/en_US.news.txt")
twitter <- readData("en_US/en_US.twitter.txt")

Corpus_Files <- list(blogs, news, twitter)
Corpus_Summary <- data.frame(TextName = c("Blogs", "News", "Twitter"),
     FileSizeMB = sapply(Corpus_Files, function(x) object.size(x)/1048576),
     LinesCount = sapply(Corpus_Files, length),
     WordCount = sapply(Corpus_Files, function(x) { sum(sapply(str_split(x, boundary("word")), length)) }),
     MaxNWordLine = sapply(Corpus_Files, function(x) { max(sapply(str_split(x, boundary("word")), length)) }))

kable(Corpus_Summary)
```

## Exploratory Analysis

The files have a enormous size to be processed by a single computer. To start the exploratory analysis is necessary to sample the files as a starting point I am going to randomly select 5% of each file and then append all together for further analysis:

```{r sampleData, cache=TRUE}
set.seed(2018)
sizepct <- 0.05

blogsS <- sample(blogs, length(blogs)*sizepct)
newsS <- sample(news, length(news)*sizepct)
twitterS <- sample(twitter, length(twitter)*sizepct)

sampleAll <- c(blogsS, newsS, twitterS)

```

I used the tm package for a Corpus creation and its functions transform the Corpus in order to remove unwanted characters like punctuation, numbers. 

```{r CorpusCreation, cache=TRUE}
corpus <- VCorpus(VectorSource(sampleAll), 
                  readerControl = list(reader = readPlain,
                                       language = "en_US"))

corpus <- tm_map(corpus, removePunctuation)
corpus <- tm_map(corpus, removeNumbers) 

corpus <- tm_map(corpus, stripWhitespace) 
corpus <- tm_map(corpus, content_transformer(tolower))

#not sure about remove StopWords so a Corpus without will be created for comparison
corpuswoSW <- tm_map(corpus, removeWords, stopwords("english"))

```

The first graph to explore the Corpus just created, is the single term, to observe the frequency occurrency for each word found in the dataset.  

### Unique Words Frequency

```{r top10words, cache=TRUE}
unigramDTM <- TermDocumentMatrix(corpus, control = list(wordLengths=c(1, Inf)))
words_frequency <- findFreqTerms(unigramDTM,lowfreq =1000)  #lowfreq should be >1 with samplesize is more than 1%
                                                           # arrays can go greater than 10Gb - MEMORY PROBLEMS

unigramFreq <- rowSums(as.matrix(unigramDTM[words_frequency,]))
unigramTotal <- sum(unigramFreq)
unigramFreq <- data.frame(word=names(unigramFreq),frequency=unigramFreq, pctSample=100*unigramFreq/unigramTotal)
unigramFreq <- arrange(unigramFreq, desc(frequency))

ggplot(head(unigramFreq, 10), aes(x = reorder(word, -frequency), y = frequency, label = paste(round(pctSample, 1), "%", sep = ""))) + 
        geom_col() + 
        geom_label()+
        labs(x = "Tokens", y = "Frequency Count & Percentage of Sample") +
        annotate("text", label = paste("Total unigram Tokens in the sample: ", unigramTotal, 
                       "\nTotal unique Tokens:", length(Terms(unigramDTM))), x = 8, y = unigramFreq$frequency[1]) +
        theme_bw()

```

We can check that the most used words are articles and prepositions, they are not much meaninful by themselves. In Natural Language Processing, these words are called "Stopwords" and are usually removed. Let's check which are the most common words without the Stopwords.

```{r top10wordswoSW, cache=TRUE}
unigramDTMwoSW <- TermDocumentMatrix(corpuswoSW)
words_frequency <- findFreqTerms(unigramDTMwoSW,lowfreq =1000)     #lowfreq should be >1 with samplesize is more than 1%
                                                                # arrays can go greater than 10Gb - MEMORY PROBLEMS

mostWordfreq <- rowSums(as.matrix(unigramDTMwoSW[words_frequency,]))
mostWordTotal <- sum(mostWordfreq)
mostWordfreq <- data.frame(word=names(mostWordfreq),frequency=mostWordfreq, pctSample=100*mostWordfreq/mostWordTotal)
mostWordfreq <- arrange(mostWordfreq, desc(frequency))

ggplot(head(mostWordfreq, 10), aes(x = reorder(word, -frequency), y = frequency, label = paste(round(pctSample, 1), "%", sep = ""))) + 
        geom_col() + 
        geom_label()+
        labs(x = "Tokens", y = "Frequency Count & Percentage of Sample") +
        annotate("text", label = paste("Total unigram Tokens in the sample: ", mostWordTotal, 
                                       "\nTotal unique Tokens:", length(Terms(unigramDTMwoSW))), x = 8, y = mostWordfreq$frequency[1]) +
        theme_bw()

```

It's intersting to check that tm package function remove "stopwords" and the difference in the total unique Tokens is 143055-14233 = 722, this is the amount of Stop words found in the sample.  
One of the simpliest models approach and Language Processing is the called N-gram where it assigns probabilities to a sequence of N words, where N can be 2 (bigram), 3 (trigram), 4 (fourgram) and so on. Let's check what is the most frequent two words combinations with and without Stopwords.  

### 2-grams Frequency  

```{r top10bigram, cache=TRUE}
Bigramtokenizer <- function(x)
        unlist(lapply(ngrams(words(x), 2), paste, collapse = " "), use.names = FALSE)

bigramDTM <- TermDocumentMatrix(corpus,control = list(tokenize = Bigramtokenizer))

lowfreq = 1000
words_frequency <- findFreqTerms(bigramDTM,lowfreq = lowfreq)

bigramFreq <- rowSums(as.matrix(bigramDTM[words_frequency,]))
bigramTotal <- sum(bigramFreq)
bigramFreq <- data.frame(word=names(bigramFreq),frequency=bigramFreq, pctSample=100*bigramFreq/bigramTotal)
bigramFreq <- arrange(bigramFreq, desc(frequency))

ggplot(head(bigramFreq, 10), aes(x = reorder(word, -frequency), y = frequency, label = paste(round(pctSample, 1), "%", sep = ""))) + 
        geom_col() +
        geom_label()+
        labs(x = "Tokens", y = "Frequency Count & Percentage of Sample") +
        annotate("text", label = paste("Total biGrams Tokens with freq > ", lowfreq, ":", bigramTotal,
                                       "\nTotal unique Tokens:", length(Terms(bigramDTM))), x = 7, y = bigramFreq$frequency[1]) +
        theme_bw()

```

```{r top10bigramwoSW, cache=TRUE}
bigramDTMwoSW <- TermDocumentMatrix(corpuswoSW,control = list(tokenize = Bigramtokenizer))

lowfreq = 400
words_frequency <- findFreqTerms(bigramDTMwoSW,lowfreq = lowfreq)

bigramFreqwoSW <- rowSums(as.matrix(bigramDTMwoSW[words_frequency,]))
bigramTotal <- sum(bigramFreqwoSW)
bigramFreqwoSW <- data.frame(word=names(bigramFreqwoSW),frequency=bigramFreqwoSW, pctSample=100*bigramFreqwoSW/bigramTotal)
bigramFreqwoSW <- arrange(bigramFreqwoSW, desc(frequency))

ggplot(head(bigramFreqwoSW, 10), aes(x = reorder(word, -frequency), y = frequency, label = paste(round(pctSample, 1), "%", sep = ""))) + 
        geom_col() +
        geom_label()+
        labs(x = "Tokens", y = "Frequency Count & Percentage of Sample") +
        annotate("text", label = paste("Total biGrams Tokens with freq > ", lowfreq, ":", bigramTotal,
                                       "\nTotal unique Tokens:", length(Terms(bigramDTMwoSW))), x = 8, y = bigramFreqwoSW$frequency[1]) +
        theme_bw()

```

At this point, we can see the bigrams with stop words does not describe much, and without we can learn that the 4th word in the list is New York, one of the biggest cities in USA.  
Therefore, to conclude the exploratory analysis, let's evaluate the trigrams and fourgrams most found in the dataset.

### 3-grams Frequency  

```{r top10trigramwoSW, cache=TRUE}
trigramtokenizer <- function(x) {
        unlist(lapply(ngrams(words(x), 3), paste, collapse = " "), use.names = FALSE) }

trigramDTM <- TermDocumentMatrix(corpuswoSW,control = list(tokenize = trigramtokenizer))

lowfreq = 50
words_frequency <- findFreqTerms(trigramDTM,lowfreq = lowfreq)

trigramFreq <- rowSums(as.matrix(trigramDTM[words_frequency,]))
trigramTotal <- sum(trigramFreq)
trigramFreq <- data.frame(word=names(trigramFreq),frequency=trigramFreq, pctSample=100*trigramFreq/trigramTotal)
trigramFreq <- arrange(trigramFreq, desc(frequency))

ggplot(head(trigramFreq, 10), aes(x = reorder(word, -frequency), y = frequency, label = paste(round(pctSample, 1), "%", sep = ""))) + 
        geom_col() + 
        geom_label() +
        labs(x = "Tokens", y = "Frequency Count & Percentage of Sample") +
        annotate("text", label = paste("Total triGrams Tokens with freq > ", lowfreq, ":", trigramTotal,
                                       "\nTotal unique Tokens:", length(Terms(trigramDTM))), 
                 x = 8, y = trigramFreq$frequency[1]) + theme_bw() +
        theme(axis.text.x = element_text(angle = 60, hjust = 1)) 
```

### 4-grams Frequency  

```{r top10fourgramwoSW, cache=TRUE}
fourgramtokenizer <- function(x) {
        unlist(lapply(ngrams(words(x), 4), paste, collapse = " "), use.names = FALSE) }

fourgramDTM <- TermDocumentMatrix(corpuswoSW,control = list(tokenize = fourgramtokenizer))

lowfreq = 10
most_words <- findFreqTerms(fourgramDTM,lowfreq = lowfreq)

fourgramFreq <- rowSums(as.matrix(fourgramDTM[most_words,]))
fourgramTotal <- sum(fourgramFreq)
fourgramFreq <- data.frame(word=names(fourgramFreq),frequency=fourgramFreq, pctSample=100*fourgramFreq/fourgramTotal)
fourgramFreq <- arrange(fourgramFreq, desc(frequency))

ggplot(head(fourgramFreq, 10), aes(x = reorder(word, -frequency), y = frequency, label = paste(round(pctSample, 2), "%", sep = ""))) + 
        geom_col() + 
        geom_label() +
        labs(x = "Tokens", y = "Frequency Count & Percentage of Sample") +
        annotate("text", label = paste("Total fourGrams Tokens with freq > ", lowfreq, ":", fourgramTotal,
                                       "\nTotal unique Tokens:", length(Terms(fourgramDTM))), 
                 x = 8, y = fourgramFreq$frequency[1]) + theme_bw() +
        theme(axis.text.x = element_text(angle = 60, hjust = 1))

```

## Model Strategy  

Now that was described the dataset, the next step is to create a predictor of words based on the user's input. The following steps will be considered:

* check the desired current word inside the most frequency in the Unigram Matrix;
* if the last 3 words in the sentence is found in the fourgram matrix, show the most frequent;
* if the last 2 words in the sentence is found in the trigram matrix, show the most frequent;
* if the last word in the sentence is found in the bigram matrix, show the most frequent;
* from the searches above, the 3 first predictions will be presented as option to the user.

The prediction algorithm will be packed in a Shiny Application to be available at internet.